{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrDavidL/learning-dhds/blob/main/Part_5_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- David Liebovitz, MD\n",
        "- Updated by Jay Manadan\n",
        "- Center for Education in Data Science and Digital Health - Institute for Artificial Intelligence in Medicine    \n",
        "- Feinberg School of Medicine - Northwestern University    \n",
        "\n",
        "[GitHub Study Guide](https://github.com/DrDavidL/learning-dhds)\n"
      ],
      "metadata": {
        "id": "7UZIB6e_5Jfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Introduction to Neural Networks for Medical Prediction\n",
        "\n",
        "Welcome! You'll soon learn how oddly powerful neural network models are. They are even referred to as \"deep learning\" when they have multiple layers and can be used to generate highly accurate predictions!\n",
        "\n",
        "To get the big picture, this notebook will help you:\n",
        "\n",
        "1.  **Prepare your data:** You'll retrieve and transform our example medical dataset for use in a neural network. (The programming code can be modified for other data sets!)\n",
        "2.  **Build a neural network:** Create a simple model using just a few lines of Python. (Don't worry - no coding knowledge required.)\n",
        "3.  **Evaluate model performance:** We’ll assess how well your model predicts diabetes, and how changing thresholds impacts accuracy.\n",
        "4. **Reflect:** You’ll review the key ideas and tools behind your model.\n",
        "\n",
        "---\n",
        "\n",
        "## **What are Neural Networks?**\n",
        "\n",
        "Neural networks are computer models inspired by how the human brain works. They consist of layers of \"neurons\" that pass information and learn from patterns in the data.\n",
        "\n",
        "We'll focus on a feedforward neural network (FNN)—ideal for binary classification tasks like predicting whether someone has diabetes or not.\n",
        "\n",
        "##**Why Use Neural Networks?**##\n",
        "Neural networks are great at detecting complex, hidden relationships in data—often outperforming traditional models. In healthcare, they can help with:\n",
        "\n",
        "- Diagnosing conditions\n",
        "\n",
        "- Predicting disease risk\n",
        "\n",
        "- Personalizing treatments\n",
        "---\n",
        "\n",
        "There are different types of neural networks suitable for different projects. For example, here is a contrast between the FNN we'll assemble below and a Recurrent Neural Network (RNN).\n",
        "\n",
        "First, our slightly simpler **FNN**:\n",
        "\n",
        "- **Processing**: Each row of data in a batch is handled on its own. The network performs matrix operations followed by activation functions—same steps for each row.   \n",
        "- **No Time Dependence:** The model doesn’t track the order of the data. You can shuffle your input rows without affecting the prediction for any single row.\n",
        "- **Example Use Case:** Predicting diabetes based on lab results for individual patients. There’s no benefit in knowing the order of patients—each case is independent.\n",
        "\n",
        "\n",
        "\n",
        "Now, for an **RNN**:\n",
        "\n",
        "- **Processing:** Each row in a sequence is processed with information from previous rows. The model “remembers” what came before by carrying over a hidden state that affects how it reads the current input.\n",
        "  - **At each time step** ( t ): The RNN uses the current input ( xₜ ) and the hidden state from the prior step ( hₜ₋₁ ). This creates a memory-like system where earlier inputs influence later outputs.\n",
        "  \n",
        "- **Temporal Dependence:** Order matters. If you shuffle the rows, the model's understanding of the sequence changes—and so will its predictions.\n",
        "- **Example Use Cases:** Predicting the next word in a sentence (this may remind you of ChatGPT—but it actually uses a transformer, which reads all input at once!). More relevant to healthcare:      \n",
        "    - **Vital Sign Monitoring:** Observing patterns in a patient’s heart rate, blood pressure, or breathing over time in an ICU to forecast conditions like sepsis or respiratory failure.\n",
        "\n",
        "Now, back to our illustrative (and useful!) FNN. Even though different neural networks serve different purposes, they all rely on the same fundamental components. So, by building a feedforward neural network here, you're learning the core concepts that power all deep learning models—including the most advanced ones.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Before We Begin: A Little Jargon**\n",
        "\n",
        "Let's quickly cover some core concepts, so you're ready to dive in:\n",
        "\n",
        "*   **Neurons/Nodes:** The building blocks of a neural network. They take in input, apply some math, and send the result forward.\n",
        "*   **Layers:** Neurons are arranged in layers. We’ll have an *input layer* (for our features, like glucose values or BMI, the ingredients used to help our prediction), one or more *hidden layers* (where the model learns patterns), and an *output layer* (providing our prediction).\n",
        "*   **Weights:** These are the model's internal settings that determine how strongly each input affects the next layer. They get adjusted during training.\n",
        "*   **Activation Functions:** These functions add non-linearity so the model can learn more complex relationships. (We’ll explain types like ReLU and Sigmoid soon.)\n",
        "*   **Training:** This is the process where the model adjusts its weights using examples with known answers, gradually learning how to make good predictions.\n",
        "*   **Classification Threshold:** A cut-off point to turn a probability into a final prediction. For example, if the model says there’s a 0.7 chance of diabetes, and your threshold is 0.6, the final prediction will be “diabetes.”\n",
        "\n",
        "Don't worry if these concepts seem unfamiliar now – we'll explore them more thoroughly as we progress! By the end of this notebook, you'll not only be able to build a neural network but also understand the underlying principles. Let's get started!"
      ],
      "metadata": {
        "id": "1E3fQRAOZaXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A Few Additional Terms and Requirements for Neural Networks**\n",
        "\n",
        "Now that you have a general understanding, let's get a bit more specific about how neural networks work. These are the key concepts that underpin the models we'll be building:\n",
        "\n",
        "1.  **Numerical Data is King!**\n",
        "    *  Neural networks operate on numbers. If your data contains text or categories (like \"Male/Female\"), we have to convert these to numbers using encoding. We’ll use binary encoding for two-category columns and one-hot encoding for columns with more.\n",
        "\n",
        "2.  **The Architecture: Layers**\n",
        "    *   Think of layers as different stages in a pipeline.\n",
        "        *   **Input Layer:**  Where features like blood pressure or BMI are fed into the network.\n",
        "        *   **Hidden Layers:** These are the workhorses! These layers do the actual learning—spotting patterns and relationships between inputs. *Many* hidden layers are often called **deep learning** models.\n",
        "        *   **Output Layer:** Produces the model’s final prediction. In our case, the output is the probability of diabetes.\n",
        "3.  **The Brain of the Operation: Weights and Biases**\n",
        "    *   **Weights:** Determine how strongly a feature influences a prediction.\n",
        "    *   **Biases:**  Act like adjustable baselines to shift a neuron’s output up or down.\n",
        "\n",
        "4.  **Math Under the Hood: Matrix Operations**\n",
        "    *   Data flows through the network using mathematical operations like dot products and sums. Don’t worry—TensorFlow and Keras handle all of that for you automatically.\n",
        "\n",
        "5.  **Adding Non-Linearity: Activation Functions**\n",
        "    *   Activation functions introduce non-linearity to the model, allowing it to learn complex patterns and relationships in the data. They are applied to the outputs of matrix operations.\n",
        "    *   Examples include ReLU, Sigmoid, Softmax, and Tanh. We'll see examples below and use a few of them in our code!\n",
        "\n",
        "6.  **Making a Prediction: Forward Propagation**\n",
        "    *   This is when data moves from the input layer through the hidden layers to the output. It happens both during training and when the model is used to make real predictions.\n",
        "\n",
        "\n",
        "\n",
        "7.  **Measuring Our Error: Loss Function**\n",
        "    * The loss function measures how far the model’s predictions are from the true answers.\n",
        "\n",
        "    * A higher loss = bigger mistakes.\n",
        "\n",
        " * We’ll use binary crossentropy, great for yes/no problems like predicting diabetes.\n",
        "\n",
        "8.  **Learning by Correcting: Backward Propagation**\n",
        "    *  This is how the model learns: It looks at the loss. It calculates how each weight/bias affected the loss (using gradients). It adjusts the weights and biases to do better next time. This process repeats for every training example until the model gets good!\n",
        "\n",
        "\n",
        "\n",
        "9.  **Controlling the Pace: Learning Rate**\n",
        "    *   This controls how big the adjustments are during training. A small learning rate means slow and careful learning; a large one is faster but riskier.\n",
        "\n",
        "10. **Organizing Training: Epochs and Batches**\n",
        "    *   **Epoch:** One complete pass through the training dataset.\n",
        "    *   **Batch:**  A smaller subset of the data used at one time to train the model.\n",
        "\n",
        "11. **Optimization Algorithms**\n",
        "    *  These decide how the weights and biases are updated. We’ll use Adam, a popular and efficient choice.\n",
        "\n",
        "12. **Avoiding Overfitting: Regularization**\n",
        "    *   Sometimes a model gets too good at the training data and performs poorly on new data. That’s called overfitting. To prevent this, we’ll use:\n",
        " *   Dropout: Temporarily turns off some neurons during training.\n",
        " *   L2 Regularization: Penalizes overly complex models.\n",
        "\n",
        "\n",
        "13. **The Power of Curves: Non-linearity**\n",
        "    *   Non-linearity is what makes neural networks powerful. Real-world data isn’t always straight or simple. Activation functions like ReLU or Sigmoid help us capture that complexity.\n",
        "\n",
        "14. **Tuning the Model: Hyperparameters**\n",
        "    *   These are parameters set *before* training, such as learning rate, the number of layers, or the number of neurons in a layer. They are crucial for the model to perform well, and often require some amount of tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## **Frequently Asked Questions (FAQs)**\n",
        "\n",
        "Let's solidify your understanding with some common questions:\n",
        "\n",
        "1.  **What exactly is a Neural Network?**\n",
        "    *   A computer model inspired by the brain, designed to learn from data and make predictions.\n",
        "\n",
        "2.  **Why the Need for Numbers?**\n",
        "    *   Neural networks only understand numbers. Categorical or text data must be converted to numeric form.\n",
        "\n",
        "3.  **How does a Neural Network Work - In Simple Terms?**\n",
        "   \n",
        " 1.  Data goes through layers.\n",
        " 2.  The model uses weights and biases to combine inputs.\n",
        " 3. Activation functions help the model focus on what matters.\n",
        " 4. The model produces a probability or prediction.\n",
        "\n",
        "\n",
        "4.  **How Does the Network Learn?**\n",
        "    *   **Forward Pass:** The network makes a prediction based on current \"rules\" (weights and biases).\n",
        "    *   **Loss Function:** This is the \"error\" that quantifies how good (or bad!) the prediction was.\n",
        "    *   **Backward Propagation:** Based on the error, the network adjusts the \"rules\" to improve for the next prediction.\n",
        "\n",
        "5.  **What makes Neural Networks Powerful?**\n",
        "    *   **Non-linearity:** Enables networks to handle complex relationships in data.\n",
        "    *   **Layers:**  Layered structure means different levels of understanding are built up (like edges → shapes → objects in image models).\n",
        "\n",
        "6.  **Why train for a long time?**\n",
        "    *   Each epoch improves the model. Batches help train efficiently and stably.\n",
        "\n",
        "7. **What Can Neural Networks be Used for?**\n",
        "    *  Tons of tasks! Medical prediction, image recognition, speech understanding, language generation—anything with patterns in data.\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary for Beginners**\n",
        "\n",
        "*   Neural networks learn by mimicking the human brain.\n",
        "*   They rely on numbers, math operations (matrix operations), and activation functions (non-linear functions) to work.\n",
        "*   Training involves making guesses, evaluating errors, and making corrections repeatedly.\n",
        "*   Through this process, neural networks can be trained to tackle complex tasks like predicting diseases or recognizing faces."
      ],
      "metadata": {
        "id": "poHcAgaIkLJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get started with predicting diabetes, let's illustrate the components of a neural network and what's happening in the hidden layers.  If the image below is viewable, no need to run the code (which generates the image)."
      ],
      "metadata": {
        "id": "mizNop0yo6co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def draw_neural_network_corrected(ax, layers, title, colors):\n",
        "    G = nx.DiGraph()\n",
        "    pos = {}\n",
        "    node_sizes = []\n",
        "    node_colors = []\n",
        "    x_offset = 0\n",
        "\n",
        "    # Ensure colors match the number of layers\n",
        "    if len(colors) < len(layers):\n",
        "        colors = colors[:len(layers)] + [colors[-1]] * (len(layers) - len(colors))\n",
        "\n",
        "    for layer_idx, layer_nodes in enumerate(layers):\n",
        "        y_offset = -(layer_nodes - 1) / 2\n",
        "        for node_idx in range(layer_nodes):\n",
        "            node_id = f\"L{layer_idx}_N{node_idx}\"\n",
        "            pos[node_id] = (x_offset, y_offset + node_idx)\n",
        "            G.add_node(node_id)\n",
        "            node_sizes.append(1000)\n",
        "            node_colors.append(colors[layer_idx])\n",
        "        if layer_idx > 0:\n",
        "            for prev_node_idx in range(layers[layer_idx - 1]):\n",
        "                prev_node_id = f\"L{layer_idx-1}_N{prev_node_idx}\"\n",
        "                for curr_node_idx in range(layer_nodes):\n",
        "                    curr_node_id = f\"L{layer_idx}_N{curr_node_idx}\"\n",
        "                    G.add_edge(prev_node_id, curr_node_id)\n",
        "        x_offset += 1\n",
        "\n",
        "    nx.draw(G, pos, ax=ax, node_size=node_sizes, node_color=node_colors, with_labels=False, edge_color=\"gray\")\n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Perceptron\n",
        "draw_neural_network_corrected(\n",
        "    axes[0],\n",
        "    layers=[2, 1, 1],\n",
        "    title=\"(a) Perceptron\",\n",
        "    colors=[\"blue\", \"orange\", \"green\"]\n",
        ")\n",
        "\n",
        "# Multi-layer Perceptron\n",
        "draw_neural_network_corrected(\n",
        "    axes[1],\n",
        "    layers=[3, 2, 1],\n",
        "    title=\"(b) Multi-layer Perceptron\",\n",
        "    colors=[\"blue\", \"orange\", \"green\"]\n",
        ")\n",
        "\n",
        "# Deep Neural Network\n",
        "draw_neural_network_corrected(\n",
        "    axes[2],\n",
        "    layers=[3, 3, 3, 1],\n",
        "    title=\"(c) Deep Neural Network\",\n",
        "    colors=[\"blue\", \"orange\", \"orange\", \"green\"]\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WqFk96MPMyJN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the left side of these neural networks, the blue dots are inputs, say, blood pressure and glucose values. Then, these are processed in the single hidden orange neuron layer, which gives either 1 or 0 reflecting either diabetes or no diabetes in the green output. This is a perceptron. However, our algorithm can work much better with more inputs feeding more processing neurons (orange) before going to an output. This is a multilayer perceptron. But, say we had more processing layers of neurons, (columns of orange neurons), this becomes a deep neural network as seen in c above."
      ],
      "metadata": {
        "id": "L2giK-t6Nr1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, what's really happening to values getting in those orange neurons? At that point, values can be silenced, magnified or altered. Rather than just on or off like 1 or 0 that might help a linear algorithm, other outputs are possible with non-linear features such as:"
      ],
      "metadata": {
        "id": "UkWRnAv9P6ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t•\tReLU: Passes positives as-is, sets negatives to 0. (E.g., 2.5 → 2.5)\n",
        "\t•\tSigmoid: Squeezes values into a range of 0 to 1. (E.g., 2.5 → 0.924)\n",
        "\t•\tTanh: Squeezes values into a range of -1 to 1. (E.g., 2.5 → 0.986)\n",
        "\t•\tSoftmax: Turns a list into probabilities that add to 1. (E.g., [1, 2, 3, 4, 5] → [0.011, 0.031, 0.085, 0.231, 0.641])"
      ],
      "metadata": {
        "id": "pBXB4rS8qBes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what the functions look like (no need to run code - it will just generate this image):"
      ],
      "metadata": {
        "id": "PMyf5FtuxAZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Define Activation Functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))  # Stabilizing softmax for numerical stability\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "# Create a range of input values\n",
        "x = np.linspace(-10, 10, 500)\n",
        "\n",
        "# Compute activation function outputs\n",
        "relu_values = relu(x)\n",
        "sigmoid_values = sigmoid(x)\n",
        "tanh_values = tanh(x)\n",
        "\n",
        "# Plot the Activation Functions\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# ReLU Plot\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(x, relu_values, label='ReLU', color='blue')\n",
        "plt.title('ReLU Activation Function')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.grid(True)\n",
        "\n",
        "# Sigmoid Plot\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(x, sigmoid_values, label='Sigmoid', color='green')\n",
        "plt.title('Sigmoid Activation Function')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.grid(True)\n",
        "\n",
        "# Tanh Plot\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(x, tanh_values, label='Tanh', color='red')\n",
        "plt.title('Tanh Activation Function')\n",
        "plt.xlabel('Input')\n",
        "plt.ylabel('Output')\n",
        "plt.grid(True)\n",
        "\n",
        "# Softmax Plot (Softmax applied to a sample vector)\n",
        "x_softmax = np.array([1.0, 2.0, 3.0, 4.0, 5.0])  # Example vector\n",
        "softmax_values = softmax(x_softmax)\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.bar(range(len(x_softmax)), softmax_values, color='purple', alpha=0.7)\n",
        "plt.title('Softmax Activation Function')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Probability')\n",
        "plt.xticks(range(len(x_softmax)), labels=[f'Cat {i}' for i in range(len(x_softmax))])\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wZO4yjX7pXOZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Activation Functions\n",
        "We mentioned earlier that activation functions are key to how a network learns. Let’s break down the most commonly used ones:\n",
        "\n",
        "1.  **ReLU (Rectified Linear Unit) Activation Function:**\n",
        "\n",
        "    *   **Functionality:** Keeps positive numbers the same and turns negatives into zero. You can see this as the \"elbow-shaped\" blue line.\n",
        "    *   **Analogy:** Think of it like a switch that only activates for a certain signal, and then provides a linear response with that signal.\n",
        "    *   **Use case:** Often used in hidden layers due to its computational efficiency, but it should be used with care.\n",
        "\n",
        "2.  **Sigmoid Activation Function:**\n",
        "\n",
        "    *   **Functionality:** Converts numbers to a range between 0 and 1. The green S-curve demonstrates this.\n",
        "    *   **Analogy:** This function can be seen as a \"probability\" generator, with output closer to 0 representing less likelihood and closer to 1 more likelihood.\n",
        "    *   **Use case:** Perfect for binary classification where output = probability (like predicting \"diabetes\" or \"no diabetes\"). Also, a good choice when you would like to prevent very large values or very small values to propagate through the network, which may be unstable.\n",
        "\n",
        "3.  **Tanh (Hyperbolic Tangent) Activation Function:**\n",
        "\n",
        "    *   **Functionality:** Converts values to a range between -1 and 1.\n",
        "    *   **Analogy:** Similar to Sigmoid but ranges from -1 to 1.\n",
        "    *   **Use case:** Can be used as a replacement for sigmoid in hidden layers but sometimes makes the network harder to train than ReLU, but does not \"turn off\" like ReLU does.\n",
        "\n",
        "4.  **Softmax Activation Function:**\n",
        "\n",
        "    *   **Functionality:** The Softmax function takes a list of raw scores (called logits) and turns them into probabilities that add up to 1. Each number in the list is transformed in a way that reflects its relative size compared to the others. Bigger numbers get bigger probabilities, but everything stays between 0 and 1. In this example, there are 5 categories (Cat 0, Cat 1, Cat 2, Cat 3, Cat 4) and for this given input into the softmax, the output is 0.01, 0.03, 0.09, 0.23, and 0.64 respectively.\n",
        "    *   **Analogy:** A softmax is like a team of people (different categories), where some are more or less likely to be involved, and those numbers always add to 1 (or 100%).\n",
        "    *   **Use case:** Commonly used in the output layer for multi-class classification problems, where you need to predict one category out of multiple categories,for example, pedicting which disease a patient has from several options, or choosing which image class (cat, dog, bird) best matches a photo.\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "*   **Non-linearity:** Activation functions add non-linearity.\n",
        "*   **Output Range:** Each function has a different range and use case.\n",
        "*   **Choice Matters:** The choice of activation function can significantly affect model performance.\n",
        "*   **We'll Use These:**  These activation functions are the primary ones we'll use as we get to the coding sections of our notebook.\n",
        "\n",
        "**Think of these functions as lenses that allow your network to focus on different aspects of the data. By using them together, the network can learn more complex and nuanced patterns.**"
      ],
      "metadata": {
        "id": "eAdaeWXzpTD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding How Neural Networks Learn: The Magic of Backpropagation**\n",
        "\n",
        "Forward propagation gets us a prediction. But how does the model get better?\n",
        "\n",
        "Answer: Backpropagation — a feedback process that adjusts the model’s internal settings.\n",
        "Here's a breakdown of what happens during this crucial step:\n",
        "\n",
        "1.  **Adjusting the Importance: Weights and Biases**\n",
        "    *   These are tweaked to increase or decrease the influence of inputs. It’s like adjusting the knobs on an old radio to get a clearer signal, or like adjusting the mix of ingredients in a recipe that isn't quite right.\n",
        "\n",
        "\n",
        "2.  **Calculating the \"Error Direction\": Gradients**\n",
        "    *   Gradients are calculated using calculus, specifically the **chain rule**. These tell us the \"direction\" to adjust weights and biases. They are essentially a mathematical way of saying \"adjust this weight or bias to *make less errors*.\"\n",
        "\n",
        "4.  **Updated Outputs: Activation Outputs**\n",
        "    *   As weights and biases change, the outputs of the activation functions will also change. This means the network's interpretation of the data and predictions evolve.\n",
        "\n",
        "5.  **Measuring Progress: Loss Value**\n",
        "    *   The loss value quantifies the \"error\". As the model learns, you'll see the loss value decreasing. This means the network is making better predictions.\n",
        "\n",
        "6.  **Advanced Learning: Optimizer States**\n",
        "    *   Sophisticated optimizers (like Adam) don't just adjust weights; they also update internal states to improve the speed and efficiency of training. Think of it like the model is \"learning how to learn.\"\n",
        "\n",
        "7.  **Preventing Over-Commitment: Regularization Penalties**\n",
        "    *   Regularization methods impose a penalty to control how complex the model becomes. This prevents \"overfitting,\" where the model memorizes the training data and performs poorly on new, unseen data. It's like making sure the model doesn't get too caught up in the details of one patient's case and can generalize to others.\n",
        "\n",
        "8.  **Adding Robustness: Dropout Configurations**\n",
        "    *   Dropout randomly deactivates some neurons during training. This forces the network to learn more robust and generalized patterns, preventing any single neuron from becoming too important. Think of it like different physicians offering independent assessments and advice.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "Backpropagation is the heart of how neural networks learn. It's a feedback loop that constantly refines the network’s \"understanding\" of the data by making small adjustments to the model's parameters.  Through this iterative process, the network becomes increasingly accurate in its predictions.\n",
        "\n",
        "Think of it this way: When a doctor makes a diagnosis, and it turns out not to be quite right, they will then update their understanding of the patient based on their new knowledge, to improve future diagnoses. Neural networks operate similarly, but with math!"
      ],
      "metadata": {
        "id": "9KItLI_xt7VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One More Analogy: Tuning a Music System**\n",
        "Think of backpropagation as tuning a music system:  \n",
        "\t•\tWeights and Biases: Like adjusting the volume knobs for each instrument.   \n",
        "\t•\tGradients: Tell you which knob to turn and how much.   \n",
        "\t•\tLoss Value: Measures how far you are from the perfect sound.   \n",
        "\t•\tDropout: Occasionally mutes instruments to ensure no one sound dominates   \n",
        "\n",
        "And - the output is a beautiful composition! (or much more accurate prediction!)\n",
        "\n",
        "*After all this background material, we are ready to  create and train our own neural network to predict diabetes from our dataset!*\n",
        "\n",
        "**Be sure to run the cell below which provides needed libraries for subsequent cells!**"
      ],
      "metadata": {
        "id": "rx_DRdEwuFXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ40K191ZKST"
      },
      "outputs": [],
      "source": [
        "# First, we install required libraries\n",
        "!pip install tensorflow pandas scikit-learn matplotlib seaborn --quiet\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's load our dataset we've been using! And, remind ourselves what's in it."
      ],
      "metadata": {
        "id": "HevneCl6qRe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we load our diabetes dataset\n",
        "url = 'https://raw.githubusercontent.com/DrDavidL/auto_analyze/refs/heads/master/data/predictdm.csv'\n",
        "diabetes_data = pd.read_csv(url)\n",
        "\n",
        "\n",
        "# Let's remind ourselves about our dataset!\n",
        "print(\"The first 5 rows of our dataset: \\n\")\n",
        "display(diabetes_data.head())  # Display the first few rows of the data\n",
        "print(\"\\n\\n Analysis of numerical variables in our dataset: \\n\")\n",
        "display(diabetes_data.describe()) # Display descriptive stats\n",
        "print(\"\\n\\n All the data types in our dataset: \\n\")\n",
        "display(diabetes_data.info())  # Display the info\n",
        "\n"
      ],
      "metadata": {
        "id": "wML1eO9sZ8y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember when we said neural networks use numbers?\n",
        "The data types in our dataset include integers, floats (decimal numbers), and… uh oh, objects! These \"object\" types—like words such as “female” or “Diabetes”—can’t be directly used by neural networks. We need to convert them to numbers.\n",
        "\n",
        "Since columns like \"Gender\" and \"Diabetes\" only have two categories, we use binary encoding. This means we assign 0 to the most common category and 1 to the less common one. For example, if \"female\" appears more than \"male\", we encode female as 0 and male as 1.\n",
        "\n",
        "(If a column had more than two unique values—like different races or job types—we’d use one-hot encoding instead, which creates separate columns for each category and fills them with 1s and 0s to represent presence or absence.)"
      ],
      "metadata": {
        "id": "bkJIzZ8WRZLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Function to find and encode binary categorical features\n",
        "def encode_binary_features(df, binary_columns):\n",
        "    encoding_details = {}\n",
        "    for col in binary_columns:\n",
        "        value_counts = df[col].value_counts()\n",
        "        if len(value_counts) == 2:  # Check if it's binary\n",
        "            most_frequent, least_frequent = value_counts.index\n",
        "            df[col] = df[col].apply(lambda x: 0 if x == most_frequent else 1)\n",
        "            encoding_details[col] = {0: most_frequent, 1: least_frequent}\n",
        "        else:\n",
        "            print(f\"Column '{col}' has more than two unique values. Consider using one-hot encoding.\")\n",
        "    return df, encoding_details\n",
        "\n",
        "# Identify categorical columns in the dataset\n",
        "categorical_columns = diabetes_data.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Encode binary categorical features\n",
        "diabetes_data, encoding_details = encode_binary_features(diabetes_data, categorical_columns)\n",
        "\n",
        "# Display encoding details\n",
        "for col, mapping in encoding_details.items():\n",
        "    print(f\"Encoding for column '{col}': {mapping}\")\n",
        "\n",
        "# Separate features and target\n",
        "X = diabetes_data.drop(columns=['Diabetes'])\n",
        "y = diabetes_data['Diabetes']\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Let's save all this pre-processing to make an app later! Here, we store the fitted scaler, the encoding details, and the list of features (column names from X)\n",
        "preprocessor = {\n",
        "    'scaler': scaler,\n",
        "    'encoding_details': encoding_details,\n",
        "    'feature_columns': list(X.columns)\n",
        "}\n",
        "\n",
        "# Save the preprocessor dictionary to a pickle file\n",
        "with open('preprocessor.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessor, f)\n",
        "\n",
        "print(\"Pre-processing pipeline and feature columns saved successfully to 'preprocessor.pkl'.\")\n"
      ],
      "metadata": {
        "id": "_le6NUHobEZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define our untrained neural network!!!\n",
        "To start, we’ll build a feedforward neural network that uses the *ReLU* (Rectified Linear Unit) activation function—great for learning complex, non-linear relationships efficiently.\n",
        "\n",
        "- Input Layer: 14 features from our dataset\n",
        "\n",
        "- Hidden Layer 1: 32 neurons, ReLU activation\n",
        "\n",
        "- Hidden Layer 2: 16 neurons, ReLU activation\n",
        "\n",
        "- Output Layer: 1 neuron with a sigmoid function (returns a probability between 0 and 1)\n",
        "\n",
        "To prevent overfitting (when a model memorizes training data and performs poorly on new data), we:\n",
        "\n",
        "- Add dropout layers, which randomly disable some neurons during training.\n",
        "\n",
        "- Use L2 regularization, which penalizes overly complex models.\n",
        "\n",
        "The result? A compact, efficient network that learns patterns while staying flexible enough to generalize to new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "W6qPpzFWTqXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to create the neural network model\n",
        "def create_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        Dropout(0.5),\n",
        "        Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "rEkQAn-xQ9Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might wonder what our freshly created neural network would look like.\n",
        "Picture it like this:\n",
        "\n",
        "- 14 blue input nodes (for each feature)\n",
        "\n",
        "- Yellow hidden neurons (32 in the first layer, 16 in the second)\n",
        "\n",
        "- 1 green output node (predicting diabetes or not)\n",
        "\n",
        "Grey lines represent the connections and weights between layers. Dropout isn’t visualized, but it’s there—silently helping the model avoid tunnel vision by shaking things up during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "o_Ls3XmaXoFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Reimport necessary libraries after code execution state reset\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def draw_model_diagram(ax, layers, layer_names, layer_params, colors):\n",
        "    \"\"\"\n",
        "    Draws a neural network diagram corresponding to the provided model summary.\n",
        "\n",
        "    Args:\n",
        "    ax: matplotlib axis to draw on.\n",
        "    layers: List of integers representing the number of nodes in each layer.\n",
        "    layer_names: List of strings representing layer names.\n",
        "    layer_params: List of integers representing parameter counts for each layer.\n",
        "    colors: List of colors for each layer.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "    pos = {}\n",
        "    node_sizes = []\n",
        "    node_colors = []\n",
        "    x_offset = 0\n",
        "\n",
        "    for layer_idx, (layer_nodes, layer_name, params) in enumerate(zip(layers, layer_names, layer_params)):\n",
        "        y_offset = -(layer_nodes - 1) / 2\n",
        "        for node_idx in range(layer_nodes):\n",
        "            node_id = f\"L{layer_idx}_N{node_idx}\"\n",
        "            pos[node_id] = (x_offset, y_offset + node_idx)\n",
        "            G.add_node(node_id)\n",
        "            node_sizes.append(30)\n",
        "            node_colors.append(colors[layer_idx])\n",
        "        if layer_idx > 0:\n",
        "            for prev_node_idx in range(layers[layer_idx - 1]):\n",
        "                prev_node_id = f\"L{layer_idx-1}_N{prev_node_idx}\"\n",
        "                for curr_node_idx in range(layer_nodes):\n",
        "                    curr_node_id = f\"L{layer_idx}_N{curr_node_idx}\"\n",
        "                    G.add_edge(prev_node_id, curr_node_id)\n",
        "        # Annotate layer name and parameter count\n",
        "        ax.text(x_offset, -layer_nodes - 1.5, f\"{layer_name}\\n{params} params\", ha=\"center\", fontsize=8)\n",
        "        x_offset += 1\n",
        "\n",
        "    nx.draw(G, pos, ax=ax, node_size=node_sizes, node_color=node_colors, with_labels=False, edge_color=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "\n",
        "# Add an input layer with 14 variables to the neural network diagram.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Updated parameters with an input layer\n",
        "layers = [14, 32, 16, 1]\n",
        "layer_names = [\"Input\", \"dense\", \"dense_1\", \"dense_2\"]\n",
        "layer_params = [0, 480, 528, 17]  # Input layer has no parameters\n",
        "colors = [\"blue\", \"orange\", \"orange\", \"green\"]\n",
        "\n",
        "draw_model_diagram(ax, layers, layer_names, layer_params, colors)\n",
        "ax.set_title(\"Diabetes Prediction Neural Network Diagram\", fontsize=14)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LgmjQ2vtYXM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our network above, the 14 inputs from our dataset are blue, the hidden layer neurons are both yellow and the output is green. All that grey shows all the connections between layers!\n",
        "\n",
        "---\n",
        "\n",
        "### **We are ready to train our model!**\n",
        "Like activation functions, there are several ways to train and evaluate our model. Choosing the right approach depends on our data, goals, and computational resources. Let's explore three options here:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Simple Train-Validation Split**\n",
        "- **How it works**:\n",
        "  - Split the dataset into two parts: training (e.g., 80%) and validation (20%).\n",
        "  - The model learns patterns from the training data over multiple **epochs** (complete passes through the training data) and is tested on the validation set after each epoch to monitor progress.\n",
        "\n",
        "- **Why use it?**\n",
        "  - It's simple and quick to implement.\n",
        "  - Provides a clear separation between the data used for training and evaluation.\n",
        "\n",
        "- **Limitations**:\n",
        "  - Performance might vary depending on how the data is split—some splits may be better than others.\n",
        "  - Risk of overfitting to the training set or underfitting the validation set.\n",
        "\n",
        "- **Best for**:\n",
        "  - Quick tests when time or computational resources are limited.\n",
        "  - Datasets with a large number of samples, where splitting the data doesn't lead to a loss of valuable information.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. K-Fold Cross-Validation**\n",
        "- **How it works**:\n",
        "  - Split the data into `k` equal parts (folds). Train the model `k` times, each time using a different fold for validation and the rest for training.\n",
        "\n",
        "- **Why use it?**\n",
        "  - Ensures that every data point gets a chance to be in the validation set exactly once.\n",
        "  - Reduces variability caused by a single random split, leading to a more reliable estimate of the model’s performance.\n",
        "\n",
        "- **Limitations**:\n",
        "  - Computationally expensive, as the model is trained `k` times.\n",
        "  - May not always be suitable for very large datasets or time-sensitive applications.\n",
        "\n",
        "- **Best for**:\n",
        "  - Medium-sized datasets when accuracy matters more than speed.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Stratified K-Fold Cross-Validation**\n",
        "- **How it works**:\n",
        "  - Like regular K-Fold, but makes sure each fold keeps the same proportion of labels.\n",
        "  - For example, if 30% of the dataset is labeled as \"positive\" and 70% as \"negative,\" each fold will maintain this 30:70 ratio.\n",
        "\n",
        "- **Why use it?**\n",
        "  - Ensures fairness when classes are imbalanced—important when, say, only 15% of patients have diabetes.\n",
        "\n",
        "- **Limitations**:\n",
        "  - Computationally expensive, like regular K-Fold Cross-Validation.\n",
        "  - The added complexity of stratification may not be necessary for balanced datasets.\n",
        "\n",
        "- **Best for**:\n",
        "  - Datasets with imbalanced classes, where preserving the class distribution during training and validation is critical.\n",
        "\n",
        "---\n",
        "\n",
        "### **Choosing the Right Approach**\n",
        "- Our dataset is small enought (390 rows and 14 columns) that we can try them all! (Larger datasets can take a long time.)\n",
        "- In general, for **reliable and robust evaluation**, especially with smaller datasets, use **K-Fold Cross-Validation**.\n",
        "- When working with **imbalanced data**, opt for **Stratified K-Fold Cross-Validation** to account for class distribution. If we had to guess - this will be best since we have many more non-diabetes patients than diabetes patients.\n",
        "\n",
        "Youll be prompted to choose a method! Give them all a try!\n",
        "\n",
        "And, one more note - for each method, many models are created and tested sometimes reaching over-fitting - where it just memorizes data. We added code to select the very best model before any obvious over-fitting, so it will look like it processes a final model once more at the end. That is when the ideal model from the approach chosen is finalized.\n",
        "\n",
        "Run this next cell to be prompted to choose a training method. (You may have to click that \"play\" icon twice.) Once you select a method, be patient since it will take a minute or so!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxhrwrgpbBW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Load and preprocess data\n",
        "X = diabetes_data.drop(columns=['Diabetes']).values  # Features\n",
        "y = diabetes_data['Diabetes'].values  # Target\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# User selection for training method\n",
        "print(\"Choose a training approach:\")\n",
        "print(\"1: Simple Train-Validation Split\")\n",
        "print(\"2: K-Fold Cross-Validation\")\n",
        "print(\"3: Stratified K-Fold Cross-Validation\")\n",
        "choice = int(input(\"Enter choice (1/2/3): \"))\n",
        "\n",
        "# Variables for logging accuracies\n",
        "val_acc = []\n",
        "train_acc = []\n",
        "optimal_epoch = 0\n",
        "\n",
        "if choice == 1:\n",
        "    # Simple Train-Validation Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model = create_model(input_shape=X_train.shape[1])\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "    # Log training/validation metrics\n",
        "    train_acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    # Find optimal epoch\n",
        "    optimal_epoch = np.argmax(val_acc) + 1\n",
        "    print(f\"Optimal number of epochs: {optimal_epoch}\")\n",
        "\n",
        "elif choice == 2:\n",
        "    # K-Fold Cross-Validation\n",
        "    from sklearn.model_selection import KFold\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_no = 1\n",
        "    all_fold_val_acc = []\n",
        "\n",
        "    for train_idx, val_idx in kfold.split(X_scaled):\n",
        "        X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Train the model\n",
        "        model = create_model(input_shape=X_train_fold.shape[1])\n",
        "        history = model.fit(X_train_fold, y_train_fold,\n",
        "                            validation_data=(X_val_fold, y_val_fold),\n",
        "                            epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "        # Log validation accuracies for fold\n",
        "        fold_val_acc = history.history['val_accuracy']\n",
        "        all_fold_val_acc.append(fold_val_acc)\n",
        "\n",
        "        print(f\"Fold {fold_no} Complete\")\n",
        "        fold_no += 1\n",
        "\n",
        "    # Aggregate validation accuracies across folds to determine optimal epoch\n",
        "    mean_val_acc = np.mean(all_fold_val_acc, axis=0)\n",
        "    optimal_epoch = np.argmax(mean_val_acc) + 1\n",
        "    print(f\"Optimal number of epochs based on mean validation accuracy: {optimal_epoch}\")\n",
        "\n",
        "elif choice == 3:\n",
        "    # Stratified K-Fold Cross-Validation\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_no = 1\n",
        "    all_fold_val_acc = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_scaled, y):\n",
        "        X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Train the model\n",
        "        model = create_model(input_shape=X_train_fold.shape[1])\n",
        "        history = model.fit(X_train_fold, y_train_fold,\n",
        "                            validation_data=(X_val_fold, y_val_fold),\n",
        "                            epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "        # Log validation accuracies for fold\n",
        "        fold_val_acc = history.history['val_accuracy']\n",
        "        all_fold_val_acc.append(fold_val_acc)\n",
        "\n",
        "        print(f\"Fold {fold_no} Complete\")\n",
        "        fold_no += 1\n",
        "\n",
        "    # Aggregate validation accuracies across folds to determine optimal epoch\n",
        "    mean_val_acc = np.mean(all_fold_val_acc, axis=0)\n",
        "    optimal_epoch = np.argmax(mean_val_acc) + 1\n",
        "    print(f\"Optimal number of epochs based on mean validation accuracy: {optimal_epoch}\")\n",
        "\n",
        "else:\n",
        "    print(\"Invalid choice. Please run the program again and choose 1, 2, or 3.\")\n",
        "\n",
        "# Step 4: Retrain Final Model Using Optimal Epoch\n",
        "print(\"\\nRetraining final model using optimal number of epochs...\")\n",
        "\n",
        "# Reserve a portion of the data for validation\n",
        "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Retrain the model using the optimal number of epochs\n",
        "final_model = create_model(input_shape=X_train_final.shape[1])\n",
        "final_history = final_model.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    validation_data=(X_val_final, y_val_final),  # Provide validation data\n",
        "    epochs=optimal_epoch, batch_size=32, verbose=1\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "final_model.save(\"final_model.keras\")\n",
        "print(f\"Final model trained with {optimal_epoch} epochs and saved as 'final_model.keras'\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N3g1zM7dHzQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK! Our model is trained! Let's see what happened. The code below will show us how well the model learned after each epoch. And how well the model at that point performed with the validation test set!\n"
      ],
      "metadata": {
        "id": "p4rOzBJ9dNWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize final training history - Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(final_history.history['accuracy'], label='Train Accuracy', color='blue')\n",
        "plt.plot(final_history.history['val_accuracy'], label='Validation Accuracy', color='orange')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Final Model Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Visualize final training history - Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(final_history.history['loss'], label='Train Loss', color='blue')\n",
        "plt.plot(final_history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Final Model Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bgt5yq3tdQ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model looks pretty good! Specifically:\n",
        "\n",
        "### **Training Accuracy Observations**\n",
        "1. **Accuracy Increase Over Epochs:**\n",
        "   - The training accuracy (blue line) steadily increases, indicating that our model is learning from the training data.\n",
        "   - The validation accuracy (orange line) also improves and stabilizes, showing that our model is generalizing well to unseen data.\n",
        "\n",
        "2. **Validation Accuracy is Higher than Training Accuracy:**\n",
        "   - This might be due to the dropout neuron regularization technique we included causing the model to perform better on the validation set (using all neurons) than on the training set.\n",
        "   - It could also be due to random effects from a well-designed validation set.\n",
        "\n",
        "3. **Convergence:**\n",
        "   - Both training and validation accuracy appear to stabilize around epoch 35-40, suggesting that the model has learned most of the patterns in the data and further training might not significantly improve performance.\n",
        "\n",
        "4. **No Overfitting:**\n",
        "   - Overfitting is indicated when the validation accuracy decreases while the training accuracy continues to increase. This plot shows no significant signs of overfitting since validation accuracy remains stable and high.\n",
        "\n",
        "\n",
        "#### **Loss History Observations**:\n",
        "1. **Train Loss** (blue line):\n",
        "   - The training loss (how \"off\" our predictions are) steadily decreases over epochs, indicating that our model is optimizing well on the training data.\n",
        "\n",
        "2. **Validation Loss** (orange line):\n",
        "   - The validation loss also decreases and closely tracks the training loss. This is a good sign that our model is generalizing well to unseen data without overfitting.\n",
        "\n",
        "3. **Convergence**:\n",
        "   - By around epoch 20–30, both the training and validation losses stabilize, suggesting that our model has reached an optimal level of learning. Further training might not result in significant improvements.\n",
        "\n",
        "4. **No Overfitting**:\n",
        "   - Overfitting again is typically indicated by a divergence between validation loss and training loss (e.g., validation loss increasing while training loss continues to decrease). This plot shows no such divergence, meaning our model is well-regularized and performs well on unseen data.\n",
        "\n",
        "#### **Conclusion**:\n",
        "Our model is well-trained and balanced, with no significant signs of overfitting or underfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "Yb8lglIZeXWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK - we’ve got a model! But how do we decide when to say “this person likely has diabetes”?\n",
        "Our model gives probabilities. Should we classify someone as diabetic if their probability is 0.30? 0.80?\n",
        "\n",
        "To find the best decision threshold, we plotted the ROC Curve (Receiver Operating Characteristic). It shows the trade-off between sensitivity (true positive rate) and 1 - specificity (false positive rate). From this, we use Youden’s Index to find the best threshold, which turned out to be 0.31."
      ],
      "metadata": {
        "id": "FjokAkYrlY1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get predicted probabilities for the validation set\n",
        "y_probs = final_model.predict(X_val_final).ravel()\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val_final, y_probs)\n",
        "\n",
        "# Calculate AUROC\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(f\"AUROC: {roc_auc:.2f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random guessing\n",
        "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
        "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Find the optimal threshold (maximizing Youden's Index)\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold (Youden's Index): {optimal_threshold:.2f}\")\n"
      ],
      "metadata": {
        "id": "IGzN1VaCY9oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our dataset is imbalanced (more non-diabetic patients), the ROC curve isn’t the full story.\n",
        "The Precision-Recall (PR) Curve is better in this case. It shows how well the model identifies positive cases (diabetes) while minimizing false alarms. This helps fine-tune the threshold even further based on what matters most—catching every diabetic patient, or avoiding false positives."
      ],
      "metadata": {
        "id": "oonD5ApXayKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get predicted probabilities for the validation set\n",
        "y_probs = final_model.predict(X_val_final).ravel()\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_val_final, y_probs)\n",
        "\n",
        "# Calculate Average Precision (AP)\n",
        "average_precision = average_precision_score(y_val_final, y_probs)\n",
        "print(f\"Average Precision (AP): {average_precision:.2f}\")\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall, precision, color='blue', label=f\"PR Curve (AP = {average_precision:.2f})\")\n",
        "plt.xlabel(\"Recall (Sensitivity)\")\n",
        "plt.ylabel(\"Precision (Positive Predictive Value)\")\n",
        "plt.title(\"Precision-Recall (PR) Curve\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Find the threshold that maximizes the F1 Score\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold_pr = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold (F1 Score): {optimal_threshold_pr:.2f}\")\n",
        "print(f\"Maximum F1 Score: {f1_scores[optimal_idx]:.2f}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B_zadE9KZSKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's view a confusion matrix to see how our final model performs with different thresholds and the ideal one we found above!"
      ],
      "metadata": {
        "id": "b2B5X6QvbLFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to calculate confusion matrix and classification report for a given threshold\n",
        "def evaluate_threshold(threshold):\n",
        "    # Adjust predictions based on the threshold\n",
        "    y_pred_threshold = (y_probs > threshold).astype(int)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_val_final, y_pred_threshold)\n",
        "\n",
        "    # Print confusion matrix and classification report\n",
        "    print(f\"Confusion Matrix (Threshold = {threshold}):\")\n",
        "    print(cm)\n",
        "\n",
        "    # Visualize confusion matrix\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Diabetes', 'Diabetes'],\n",
        "                yticklabels=['No Diabetes', 'Diabetes'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix (Threshold = {threshold})')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Classification Report (Threshold = {threshold}):\")\n",
        "    print(classification_report(y_val_final, y_pred_threshold))\n",
        "\n",
        "# Test with different thresholds, including the optimal threshold\n",
        "thresholds = [0.3, 0.5, optimal_threshold, 0.7]\n",
        "print(\"\\nEvaluating with different thresholds:\")\n",
        "for t in thresholds:\n",
        "    evaluate_threshold(t)\n"
      ],
      "metadata": {
        "id": "vJjFwXXxbRde",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You created a neural network, trained a model to predict diabetes and assessed different thresholds to determine the impact on diabetes detection! Once you have a model, though, you might want to save it and post it to a website for users to access! In the code above, we already took care of it. You can see this file and the pre-processing file if you click the folder icon on the left side of the window. Your personally generated neural network model, after all your hard work above, is available to you:\n",
        "\n",
        "```\n",
        "final_model.keras\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHNGl2_Hx8If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may download it to your local machine for future use by hovering over the file name, and then clicking the three dots to reveal the download option!"
      ],
      "metadata": {
        "id": "6R32JIKIUcbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, here is how you could then load your model:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model('path/to/your_model.keras')\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nB71vZrdzo0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait - can't I run my model? That is, enter values and see the predictions? Is a web app possible?"
      ],
      "metadata": {
        "id": "4MCNUjSrx08j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes!! I've created a web app that allows a user to upload a neural network model and the pre-processing steps! Go [here](https://run-nn.streamlit.app) to run your freshly created neural network and enter values!"
      ],
      "metadata": {
        "id": "GJ4hroAYyHwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, if you only chose one training method above, remember to try the others, too, by re-running that scell and selecting another training mehod!\n",
        "\n"
      ],
      "metadata": {
        "id": "Hfs6jLd6biBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "In this Colab notebook you have explored the foundational concepts of deep learning and neural networks. By integrating these concepts with real-world medical data, you should now:\n",
        "\n",
        "1. Understand the basic architecture of neural networks and how they learn from data.\n",
        "2. Recognize the significance of activation functions, optimization algorithms, and loss functions in training neural networks.\n",
        "3. Appreciate the potential of deep learning in analyzing complex medical data, such as imaging or genomic data.\n",
        "4. Gain practical insights into implementing and evaluating neural networks using Python and TensorFlow.\n",
        "\n",
        "This knowledge empowers you to critically assess and potentially contribute to innovations in healthcare AI, enhancing patient care through data-driven insights. Feel free to reach out! DL\n"
      ],
      "metadata": {
        "id": "QUDxkmBH65wf"
      }
    }
  ]
}