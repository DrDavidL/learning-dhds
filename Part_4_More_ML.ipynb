{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "generative_ai_disabled": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrDavidL/learning-dhds/blob/main/Part_4_More_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cpUyiFUIGUC"
      },
      "source": [
        "# Part 4: A bit deeper into ML\n",
        "\n",
        "- Author: David Liebovitz, MD\n",
        "- Updated by Jay Manadan\n",
        "- For Northwestern University Feinberg School of Medicine  \n",
        "- May use with attribution\n",
        "\n",
        "[GitHub Study Guide](https://github.com/DrDavidL/learning-dhds)\n",
        "\n",
        "# Getting libraries and data ready!\n",
        "\n",
        "Run the first code cell to check if our cloud computer has a GPU. If not, we'll use the regular CPU. This step also sets up the software we need.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-16T14:44:50.874881Z",
          "start_time": "2019-06-16T14:44:38.616867Z"
        },
        "id": "CbVE99Ix34sv"
      },
      "source": [
        "# === Step 1: Check GPU Availability and Set Index URL ===\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"✅ GPU Detected: {gpu_name} | CUDA {torch.version.cuda}\")\n",
        "    TORCH_INDEX_URL = \"https://download.pytorch.org/whl/cu124\"\n",
        "else:\n",
        "    print(\"⚠️ No GPU detected. Installing CPU-only versions.\")\n",
        "    TORCH_INDEX_URL = \"https://download.pytorch.org/whl/cpu\"\n",
        "\n",
        "# === Step 2: Install Required Packages ===\n",
        "!pip install -q --no-cache-dir mljar-scikit-plot imbalanced-learn shap \\\n",
        "    xgboost torch torchvision torchaudio timm opencv-python matplotlib torchxrayvision \\\n",
        "    --extra-index-url $TORCH_INDEX_URL\n",
        "\n",
        "# === Step 3: Import Libraries ===\n",
        "# -- Essential Data Science Libraries --\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as ply  # For interactive plotting\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# -- PyTorch and Image Processing --\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import cv2\n",
        "import torchxrayvision as xrv\n",
        "from skimage import io\n",
        "\n",
        "# -- Scikit-learn: Model Training, Preprocessing, and Evaluation --\n",
        "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn import metrics, svm\n",
        "from sklearn.metrics import (roc_curve, roc_auc_score, precision_recall_curve, f1_score, auc, log_loss,\n",
        "                             recall_score, precision_score, average_precision_score, classification_report,\n",
        "                             accuracy_score, RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix, ConfusionMatrixDisplay)\n",
        "\n",
        "# -- Visualization of Model Metrics --\n",
        "import scikitplot as skplt\n",
        "from scikitplot.metrics import plot_roc_curve, plot_precision_recall_curve, plot_confusion_matrix\n",
        "\n",
        "# -- XGBoost and SHAP --\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "\n",
        "# -- Handling Imbalanced Data --\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# -- Google Colab-Specific Settings --\n",
        "try:\n",
        "    from google.colab import data_table\n",
        "    data_table.enable_dataframe_formatter()\n",
        "except ImportError:\n",
        "    print(\"Not in Google Colab, skipping interactive data table formatting.\")\n",
        "\n",
        "# -- tqdm Fix for Notebooks --\n",
        "import tqdm\n",
        "tqdm.tqdm = tqdm.notebook.tqdm\n",
        "\n",
        "# -- Plotly Notebook Initialization --\n",
        "import plotly.offline\n",
        "plotly.offline.init_notebook_mode(connected=True)\n",
        "\n",
        "# === Step 4: Configure Display and Plot Settings ===\n",
        "pd.options.display.max_columns = 50\n",
        "pd.options.display.max_rows = 30\n",
        "sns.set(rc={'figure.figsize': (12, 6)})\n",
        "\n",
        "print(\"✅ Setup Complete. Ready to analyze data!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "vhP7XthF34sx"
      },
      "source": [
        "\n",
        "# Data import\n",
        "\n",
        "As in prior notebooks, this notebook utilizes a dataset sourced from University of Virginia studies by Dr. Robert Schorling of several hundred rural African American patients. Additional information on this data set is available here:   \n",
        "> https://hbiostat.org/data/repo/diabetes.html   \n",
        "\n",
        "Using this data source, [Dr. Robert Hoyt](https://data.world/rhoyt) assigned patients to a diabetes category if their hemoglobin A1c values were [6.5 or greater](https://data.world/informatics-edu/diabetes-prediction). The dataset  explored here is the modified version from Dr. Hoyt, almost ready to go for our explorations.\n",
        "\n",
        "Our next step: Let's retrieve the data to use in our Notebook!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmJEfaem34sx"
      },
      "source": [
        "# Run this cell to reference the website that is holding the diabetes data in a CSV (comma separated values) file!\n",
        "website = \"https://drive.google.com/uc?export=download&id=1PQM8eQnQpaJwe9mAVb_XBpZoWoA5nTlM\"\n",
        "\n",
        "# polyp example\n",
        "website_no = 'https://raw.githubusercontent.com/DrDavidL/dhds/main/colon_path_pred.csv'\n",
        "\n",
        "# The command below assigns the name dm_raw to the now read CSV file retrieved from the website!\n",
        "df = pd.read_csv(website)\n",
        "\n",
        "# Let's view 10 rows of data and enable sorting for the columns. Click column headers to get a feel for the max/min for columns.\n",
        "# What's the maximum SBP? Lowest HDL? Note the Filter option at the top right. Click and filter to see how many patients are over 85.\n",
        "data_table.DataTable(df, include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH6MHebyMSEV"
      },
      "source": [
        "# Encode 'Gender' and 'Diabetes' columns\n",
        "df['Gender'] = df['Gender'].map({'male': 1, 'female': 0})\n",
        "df['Diabetes'] = df['Diabetes'].map({'Diabetes': 1, 'No diabetes': 0})\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_df = df.corr()\n",
        "\n",
        "# Sort correlations with 'Diabetes' column in descending order and round to 3 decimal places\n",
        "diabetes_corr = corr_df['Diabetes'].round(3).sort_values(ascending=False)\n",
        "\n",
        "# Print correlations with presence of diabetes for each variable\n",
        "print(diabetes_corr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll change the text labels like \"male\" or \"Diabetes\" into numbers so our computer can work with them.\n",
        "Then, we calculate how strongly each variable is connected to having diabetes. For example, glucose and age are expected to have strong relationships. But others might have more complex patterns that aren't as obvious. That's where machine learning can help!"
      ],
      "metadata": {
        "id": "6V2M0hpS3KJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **XGBoost to the Rescue!**\n",
        "\n",
        "**Why Use XGBoost for Predicting Diabetes?**\n",
        "\n",
        "\n",
        "\n",
        "XGBoost is a powerful ML algorithm that works great for health data stored in tables. Here’s why:\n",
        "\n",
        "- Handles Tables Well: Perfect for data like blood pressure, cholesterol, etc.\n",
        "\n",
        "- Deals with Missing Values: You don’t need to fill in missing spots manually.\n",
        "\n",
        "- Improves Itself: Learns from mistakes in each round to get better predictions.\n",
        "\n",
        "- Avoids Overfitting: Uses smart rules to not memorize the data.\n",
        "\n",
        "- Fast and Efficient: Works well even with big datasets.\n",
        "\n",
        "- Handles Unbalanced Data: Good at spotting rare conditions (like diabetes).\n",
        "\n",
        "- Explains Itself: Shows which features (like glucose) are most important.\n",
        "\n"
      ],
      "metadata": {
        "id": "2CIwb_tEwJNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Before using a machine learning algorithm, there are some important settings, e.g., if it iterates over data, how much data should it use at a time? How many features for each pass of the data should be included? How much should the model evolve if a wrong answer is predicted? These configuration settings are inserted into algorithms together with the data to generate a predictive model. For our first pass, we'll use what are regarded as reasonable starting settings. Then, we'll apply a method to pick the best hyperparameters and see if it does better!"
      ],
      "metadata": {
        "id": "ken1iHQVDBqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define starting XGBoost hyperparameters: Configuration settings before starting!\n",
        "xgb_params = {\n",
        "    'objective': 'binary:logistic',  # For our case, specify binary classification, i.e., diabetes or not diabetes.\n",
        "    'eval_metric': 'auc',  # Evaluation metric we'll use to assess the model performance\n",
        "    'learning_rate': 0.1,  # Step size for each iteration - how much to learn with each step\n",
        "    'max_depth': 6,  # Maximum tree depth (branches)\n",
        "    'n_estimators': 100,  # Number of boosting rounds (trees)\n",
        "    'subsample': 0.8,  # Randomly sample 80% of dataset for each tree\n",
        "    'colsample_bytree': 0.8,  # Randomly sample 80% of features for each tree\n",
        "    'lambda': 1,  # L2 regularization: Reduces overfitting while still allowing all features to contribute.\n",
        "    'alpha': 0.5,  # L1 regularization: Encourages sparsity by driving some coefficients to exactly zero, effectively performing feature selection.\n",
        "    'random_state': 42  # For reproducibility: To obtain similar results when processing the data again\n",
        "}\n"
      ],
      "metadata": {
        "id": "-y19qmJtvDWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-Step Plan to Use XGBoost**\n",
        "\n",
        "1. Convert our text labels into numbers.\n",
        "\n",
        "2. Separate our target column (\"Diabetes\") from the rest.\n",
        "\n",
        "3. Set up XGBoost with some starting settings (called hyperparameters).\n",
        "\n",
        "4. Train the model on part of the data and test it on the rest.\n",
        "\n",
        "5. Check how well it did.\n",
        "\n",
        "6. Show which features mattered most.\n",
        "\n",
        "7. Look at what the model got right and wrong (confusion matrix)."
      ],
      "metadata": {
        "id": "bVgfXaqwD5TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Encode categorical variables (e.g., Gender, Diabetes)\n",
        "le = LabelEncoder()\n",
        "df['Gender'] = le.fit_transform(df['Gender'])\n",
        "df['Diabetes'] = le.fit_transform(df['Diabetes'])\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Diabetes', axis=1)\n",
        "y = df['Diabetes']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize XGBoost model with specified hyperparameters\n",
        "xgb_model = XGBClassifier(**xgb_params)\n",
        "\n",
        "# Train the model using the train set\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Train predictions and AUC score\n",
        "y_train_pred_proba = xgb_model.predict_proba(X_train)[:, 1]  # Probability for class 1\n",
        "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "\n",
        "# Test predictions and AUC score\n",
        "y_test_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
        "test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
        "\n",
        "# Generate ROC curves for train and test datasets\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
        "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_proba)\n",
        "\n",
        "# Plot ROC curves for train and test datasets\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_train, tpr_train, label=f'Train ROC (AUC = {train_auc:.2f})', linestyle='--')\n",
        "plt.plot(fpr_test, tpr_test, label=f'Test ROC (AUC = {test_auc:.2f})')\n",
        "\n",
        "# Plot random chance line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance (AUC = 0.50)')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC-AUC Curve for Train and Test Sets')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Print classification report for the test set\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print ROC-AUC score for the test set\n",
        "print(f\"Test ROC-AUC: {test_auc:.2f}\")\n",
        "\n",
        "# Extract feature importance values\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "features = X.columns\n",
        "\n",
        "# Create a DataFrame to sort the features by importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Visualize Feature Importance (Sorted)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature at the top\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"XGBoost Feature Importance (Sorted)\")\n",
        "plt.show()\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plotting Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "cax = ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.6)\n",
        "plt.colorbar(cax)\n",
        "\n",
        "for (i, j), value in np.ndenumerate(conf_matrix):\n",
        "    ax.text(j, i, f'{value}', ha='center', va='center', fontsize=16)\n",
        "\n",
        "ax.set_xlabel('Predicted Labels')\n",
        "ax.set_ylabel('True Labels')\n",
        "ax.set_title('Confusion Matrix for Test Set')\n",
        "plt.xticks([0, 1], ['No Diabetes', 'Diabetes'])\n",
        "plt.yticks([0, 1], ['No Diabetes', 'Diabetes'])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Returning the confusion matrix for reference\n",
        "conf_matrix"
      ],
      "metadata": {
        "id": "RI7CL1DV8K0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be sure to scroll the full lengthy output above. Hard to get a better ROC! But, if you look at the confusion matrix, we likely missed a couple cases of diabetes. Let's see if we can tune our hyperparameters and do even better! Instead of guessing the best settings, we’ll try out many combinations and let the computer pick the best ones using something called Random Search. This may take 30 seconds or so."
      ],
      "metadata": {
        "id": "-xL32kLhF0nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define hyperparameter grid for XGBoost\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],  # Control convergence speed\n",
        "    'max_depth': [3, 6, 9],  # Complexity of each tree\n",
        "    'n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
        "    'subsample': [0.6, 0.8, 1.0],  # Data sampling ratio per tree\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],  # Feature sampling ratio per tree\n",
        "    'lambda': [0.5, 1, 1.5],  # L2 regularization\n",
        "    'alpha': [0, 0.5, 1]  # L1 regularization\n",
        "}\n",
        "\n",
        "# Set up Random Search with 3-fold cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,  # Number of parameter settings to sample\n",
        "    scoring='roc_auc',  # AUC score as the metric\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    verbose=1,\n",
        "    n_jobs=-1,  # Use all available cores\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform the random search\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve the best model from random search\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")"
      ],
      "metadata": {
        "id": "A03tAk2rxOa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing the Best Cut-Off for Predictions**\n",
        "Once we have our best model, we need to decide what probability value to use to say \"Yes, this person likely has diabetes.\" We'll use a method called Youden's J statistic to find the best threshold.\n",
        "\n",
        "Then we:\n",
        "\n",
        "- Show the updated performance.\n",
        "\n",
        "- Look again at which features were most important.\n",
        "\n",
        "- Show the updated confusion matrix."
      ],
      "metadata": {
        "id": "GlICnlLrw0nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Use the best model to make probability predictions\n",
        "y_train_pred_proba = best_model.predict_proba(X_train)[:, 1]\n",
        "y_test_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute AUC scores\n",
        "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
        "\n",
        "# Generate ROC curves for train and test datasets\n",
        "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred_proba)\n",
        "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_pred_proba)\n",
        "\n",
        "# Compute Youden's J statistic on the TRAINING set\n",
        "J_scores_train = tpr_train - fpr_train\n",
        "best_threshold_index_train = J_scores_train.argmax()\n",
        "best_threshold_train = thresholds_train[best_threshold_index_train]\n",
        "\n",
        "print(f\"Optimal Cut-off Threshold (Youden's J from Training Set): {best_threshold_train:.3f}\")\n",
        "\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_train, tpr_train, label=f'Train ROC (AUC = {train_auc:.2f})', linestyle='--')\n",
        "plt.plot(fpr_test, tpr_test, label=f'Test ROC (AUC = {test_auc:.2f})')\n",
        "\n",
        "# Plot random chance line\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Chance (AUC = 0.50)')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC-AUC Curve for Train and Test Sets')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Predict labels using the optimal threshold\n",
        "y_pred_optimal = (y_test_pred_proba >= best_threshold_train).astype(int)\n",
        "\n",
        "# Print classification report using the optimal threshold\n",
        "print(\"Classification Report (Optimal Threshold):\")\n",
        "print(classification_report(y_test, y_pred_optimal))\n",
        "\n",
        "# Print ROC-AUC score for the test set\n",
        "print(f\"Test ROC-AUC: {test_auc:.2f}\")\n",
        "\n",
        "# Extract feature importance values\n",
        "feature_importance = best_model.feature_importances_\n",
        "features = X.columns\n",
        "\n",
        "# Create a DataFrame to sort the features by importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Visualize Feature Importance (Sorted)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature at the top\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"XGBoost Feature Importance (Sorted)\")\n",
        "plt.show()\n",
        "\n",
        "# Generate Confusion Matrix using the optimal threshold\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_optimal)\n",
        "\n",
        "# Plotting Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "cax = ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.6)\n",
        "plt.colorbar(cax)\n",
        "\n",
        "for (i, j), value in np.ndenumerate(conf_matrix):\n",
        "    ax.text(j, i, f'{value}', ha='center', va='center', fontsize=16)\n",
        "\n",
        "ax.set_xlabel('Predicted Labels')\n",
        "ax.set_ylabel('True Labels')\n",
        "ax.set_title('Confusion Matrix for Test Set (Optimal Threshold)')\n",
        "plt.xticks([0, 1], ['No Diabetes', 'Diabetes'])\n",
        "plt.yticks([0, 1], ['No Diabetes', 'Diabetes'])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Returning the confusion matrix for reference\n",
        "conf_matrix\n"
      ],
      "metadata": {
        "id": "PcvQ5o-Qwhxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the SHAP Explainer\n",
        "explainer = shap.Explainer(best_model, X_train)\n",
        "\n",
        "# Calculate SHAP values for the test set\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Plot summary plot for feature importance (global interpretation)\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "# Optional: Detailed summary plot for individual contributions\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKLqDRXnnysx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP Values: Why Did the Model Say That?**\n",
        "\n",
        "SHAP values explain individual predictions by showing how much each feature contributed. We show:\n",
        "\n",
        "- A summary bar plot of most important features.\n",
        "\n",
        "- A detailed view showing how each feature affected each patient's prediction."
      ],
      "metadata": {
        "id": "A87zQRMB9qLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imaging! If time - let's dive into images!\n",
        "\n",
        "We'll now try to predict findings on a chest xray. We'll:\n",
        "\n",
        "1. Retrieve a sample CXR.\n",
        "2. Demonstrate how we could convert it to numbers and begin a prediction process.\n",
        "3. Since creating a model can take lots of images and many minutes - we'll jump over and use a model Stanford created.\n",
        "4. We'll see what that model thinks of our sample CXR!"
      ],
      "metadata": {
        "id": "tR0kF_H-KzHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define the image URL\n",
        "url = \"https://radiologyassistant.nl/assets/chest-x-ray-heart-failure/a509797a678f8e_CHF-1c1.jpg\"  # Example Chest X-ray URL\n",
        "\n",
        "# Load the image from the URL\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content)).convert('L')  # Convert to grayscale\n",
        "\n",
        "# Resize to (224, 224) and convert to a NumPy array\n",
        "image_resized = np.array(image.resize((224, 224)))\n",
        "\n",
        "# Convert to PyTorch tensor and normalize to [0, 1]\n",
        "image_tensor = torch.tensor(image_resized).float().unsqueeze(0).unsqueeze(0) / 255.0  # Shape: (1, 1, 224, 224)\n",
        "\n",
        "# Display the Tensor Shape and Values\n",
        "print(\"Tensor Shape:\", image_tensor.shape)  # (1, 224, 224) for grayscale\n",
        "print(\"Tensor Values:\\n\", image_tensor)\n",
        "\n",
        "# Visualize the Image and Verify the Tensor Representation\n",
        "plt.imshow(image_tensor.squeeze(), cmap='gray')\n",
        "plt.title('CXR Image Tensor Representation')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r0IJaVwyNaEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above you see the CXR and on top its corresponding \"tensor\" representation. The tensor includes the sorted values for each pixel. We reduced the CXR size to 224x224 pixels. You can still see it looks OK, albeit lower resolution. Then, each of the 224x224 = 50,176 pixels is then listed sequentially in the tensor. The value is the grayscale color of that pixel! Now you see how we are directly representing that image using numbers! We are ready to analyze!"
      ],
      "metadata": {
        "id": "tE8HwpsqLXn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a CNN (Convolutional Neural Network)\n",
        "\n",
        "This is a bit beyond the scope (and processing power available for free), but we'll illustrate here how one (say of hundreds of sample CXRs) could be processed by one layer of a CNN. This will show multiple different ways to look at this CXR. Imagine a model trained on many CXRs with different filters highlighting different aspects and then mapping those features into a model to predict diagnoses. This is a glimpse into this process. Then, we'll cut to the chase (given our processing constraints), and apply a ready-to-go model to our CXR example for a prediction."
      ],
      "metadata": {
        "id": "02EzEDmXOdTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = torch.nn.Linear(16 * 112 * 112, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1_out = self.conv1(x)\n",
        "        pooled_out = self.pool(torch.nn.functional.relu(conv1_out))\n",
        "        flattened = pooled_out.view(-1, 16 * 112 * 112)\n",
        "        output = self.fc1(flattened)\n",
        "        return conv1_out, pooled_out, output\n",
        "\n",
        "# Initialize the model\n",
        "simple_cnn = SimpleCNN()\n",
        "\n",
        "# Get intermediate layer outputs\n",
        "conv1_out, pooled_out, output = simple_cnn(image_tensor)\n",
        "\n",
        "# print(\"Model Output:\", output)\n",
        "\n",
        "def plot_feature_maps(feature_map, title):\n",
        "    \"\"\"Plot the feature maps from a layer.\"\"\"\n",
        "    num_filters = feature_map.shape[1]\n",
        "    fig, axes = plt.subplots(1, num_filters, figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_filters):\n",
        "        axes[i].imshow(feature_map[0, i].detach().numpy(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot feature maps from the first convolutional layer\n",
        "plot_feature_maps(conv1_out, \"Feature Maps - Conv1 Layer\")\n"
      ],
      "metadata": {
        "id": "5MTLxoWp9Ztr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above you can see one level of how a CXR would be processed in a CNN to feed a model for analysis to predict a diagnosis.\n",
        "\n",
        "Now, let's tee up the TorchXRayVision model!"
      ],
      "metadata": {
        "id": "D_6yjkBVPYyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare the image from URL!\n",
        "url = \"https://radiologyassistant.nl/assets/chest-x-ray-heart-failure/a509797a678f8e_CHF-1c1.jpg\"  # Replace with actual image URL\n",
        "response = requests.get(url)\n",
        "img = io.imread(BytesIO(response.content))\n",
        "\n",
        "# Normalize the image to the expected range [-1024, 1024]\n",
        "img = xrv.datasets.normalize(img, 255)\n",
        "img = img.mean(2)[None, ...]  # Make single color channel\n",
        "\n",
        "# Set XRayResizer to use OpenCV (cv2) engine for better performance\n",
        "transform = torchvision.transforms.Compose([\n",
        "    xrv.datasets.XRayCenterCrop(),\n",
        "    xrv.datasets.XRayResizer(224, engine=\"cv2\")  # Use cv2 engine for faster resizing\n",
        "])\n",
        "\n",
        "# Apply the transformations\n",
        "img = transform(img)\n",
        "img = torch.from_numpy(img).float()\n",
        "\n",
        "# Load the DenseNet model with pre-trained weights\n",
        "model = xrv.models. (weights=\"densenet121-res224-all\")\n",
        "\n",
        "# Process the image with the model\n",
        "outputs = model(img[None, ...])\n",
        "\n",
        "# Convert results into a dictionary and sort by probability (descending)\n",
        "results = dict(zip(model.pathologies, outputs[0].detach().numpy()))\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display the sorted results\n",
        "print(\"Predicted Pathologies (Sorted by Probability):\")\n",
        "for pathology, probability in sorted_results:\n",
        "    print(f\"{pathology}: {probability:.4f}\")\n"
      ],
      "metadata": {
        "id": "VAdixG-raUOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not perfect - but can run on a midling CPU! Ensemble models (combining many algorithms) perform better!\n",
        "\n",
        "Hope you now have a bit of a better idea about applied ML and AI approaches!\n",
        "\n"
      ],
      "metadata": {
        "id": "J68dzaF3q0ns"
      }
    }
  ]
}