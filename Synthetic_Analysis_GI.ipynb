{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOg+TTX0Cqiggo411/uf+sS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrDavidL/learning-dhds/blob/main/Synthetic_Analysis_GI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome Data Science and AI Learners!\n",
        "\n",
        "We'll use this notebook to:\n",
        "\n",
        "1. Generate synthetic data with help of AI!\n",
        "2. Generate a predictive model.\n",
        "3. Review measures of performance of the model!"
      ],
      "metadata": {
        "id": "fhKCOdAWWHNE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58032c84"
      },
      "source": [
        "\n",
        "# Step 1: Generate Synthetic Data\n",
        "\n",
        "Let's pretend we're working on a project to predict gastrointestinal conditions. We'll ask the enterprise data warehouse team for data elements relevant to gut health.\n",
        "\n",
        "In the meantime, though, we really want to get started! Our research proposal is being reviewed and we want to be ready for when we get the real data. So, let's create a processing pipeline so we're ready when we get the real data! The first thing we'll need is synthetic data that includes potential predictors for Irritable Bowel Syndrome (IBS), which will be our target variable. Here is how we can do this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6YuLmyUWFYU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "n = 1000  # number of rows\n",
        "\n",
        "# ---- Base data (your original) ----\n",
        "data = {\n",
        "    \"Age\": np.random.normal(loc=45, scale=10, size=n).astype(int),\n",
        "    \"Sex_at_birth\": np.random.choice([\"Male\", \"Female\"], size=n),\n",
        "    \"BMI\": np.random.normal(loc=25, scale=4, size=n).round(1),\n",
        "    \"Fiber_Intake\": np.random.normal(loc=20, scale=8, size=n).round(1), # grams/day\n",
        "    \"Alcohol_Consumption\": np.random.choice([\"None\", \"Moderate\", \"Heavy\"], size=n, p=[0.5, 0.4, 0.1]), # Categories\n",
        "    \"Smoking_Status\": np.random.choice([\"Never\", \"Former\", \"Current\"], size=n, p=[0.6, 0.2, 0.2]), # Categories\n",
        "    \"Stress_Level\": np.random.randint(1, 10, size=n), # Scale 1-10\n",
        "    \"Exercise_Frequency\": np.random.choice([\"None\", \"Occasional\", \"Regular\"], size=n, p=[0.3, 0.4, 0.3]), # Categories\n",
        "    \"Presence_of_IBS\": np.random.choice([0, 1], size=n, p=[0.7, 0.3]), # Target variable (0: No IBS, 1: IBS)\n",
        "    \"Stool_Frequency\": np.random.poisson(lam=1.5, size=n) + 1 # Stool frequency (counts per day, add 1 to avoid 0)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# ---- Nudge related features by IBS status ----\n",
        "ibs_mask = df[\"Presence_of_IBS\"] == 1\n",
        "non_ibs_mask = ~ibs_mask\n",
        "\n",
        "# Stress Level: higher for IBS\n",
        "df.loc[ibs_mask, \"Stress_Level\"] = np.random.randint(5, 10, size=ibs_mask.sum())\n",
        "\n",
        "# Stool Frequency: Can be higher or lower for IBS, make it more variable\n",
        "df.loc[ibs_mask, \"Stool_Frequency\"] = np.random.poisson(lam=2.5, size=ibs_mask.sum()) + 1\n",
        "\n",
        "# Fiber Intake: slightly lower for IBS\n",
        "df.loc[ibs_mask, \"Fiber_Intake\"] = np.random.normal(loc=15, scale=6, size=ibs_mask.sum()).round(1)\n",
        "\n",
        "# ---- Clinical bounds (clips) ----\n",
        "df[\"Age\"] = df[\"Age\"].clip(18, 80)\n",
        "df[\"BMI\"] = df[\"BMI\"].clip(15, 45)\n",
        "df[\"Fiber_Intake\"] = df[\"Fiber_Intake\"].clip(5, 40)\n",
        "df[\"Stress_Level\"] = df[\"Stress_Level\"].clip(1, 10)\n",
        "df[\"Stool_Frequency\"] = df[\"Stool_Frequency\"].clip(1, 7)\n",
        "\n",
        "\n",
        "# Peek\n",
        "display(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we create a model - let's explore what we have briefly:"
      ],
      "metadata": {
        "id": "jI20to9XTLDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[\"Stool_Frequency\"], bins=range(0, df[\"Stool_Frequency\"].max()+2), kde=False)\n",
        "plt.title(\"Distribution of Stool Frequency\")\n",
        "plt.xlabel(\"Stool Frequency (counts per day)\")\n",
        "plt.ylabel(\"Count of Patients\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fXP_OJwbTPHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[\"Age\"], bins=20, kde=True)\n",
        "plt.title(\"Age Distribution\")\n",
        "plt.xlabel(\"Age (years)\")\n",
        "plt.ylabel(\"Count of Patients\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gyvjWEwZTYNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[\"BMI\"], bins=20, kde=True)\n",
        "plt.title(\"BMI Distribution\")\n",
        "plt.xlabel(\"BMI\")\n",
        "plt.ylabel(\"Count of Patients\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JuvTTU79TctH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Age boxplot\n",
        "plt.subplot(1,3,1)\n",
        "sns.boxplot(y=df[\"Age\"])\n",
        "plt.title(\"Age Distribution\")\n",
        "\n",
        "# BMI boxplot\n",
        "plt.subplot(1,3,2)\n",
        "sns.boxplot(y=df[\"BMI\"])\n",
        "plt.title(\"BMI Distribution\")\n",
        "\n",
        "# Fiber Intake boxplot\n",
        "plt.subplot(1,3,3)\n",
        "sns.boxplot(y=df[\"Fiber_Intake\"])\n",
        "plt.title(\"Fiber Intake Distribution\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "64TulNVZTt5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_stress = df.groupby(\"Presence_of_IBS\")[\"Stress_Level\"].mean().astype(int)\n",
        "print(mean_stress)\n",
        "\n",
        "# Barplot for clarity\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.barplot(x=\"Presence_of_IBS\", y=\"Stress_Level\", data=df, ci=None)\n",
        "plt.xticks([0,1], [\"No IBS\", \"IBS\"])\n",
        "plt.title(\"Mean Stress Level by IBS Status\")\n",
        "plt.ylabel(\"Mean Stress Level (1-10)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A15LaeZET6BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af72db32"
      },
      "source": [
        "# üîπ Step 1: Guidance on Algorithm Selection\n",
        "\n",
        "When deciding on a machine learning algorithm for your dataset, think about three key things:\n",
        "\n",
        "### 1. **Type of Outcome (Target Variable)**\n",
        "\n",
        "* **Binary Classification** (Yes/No, 0/1): e.g., *Presence_of_DM*, *Presence_of_HTN*.\n",
        "* **Multiclass Classification**: e.g., cancer staging, disease type.\n",
        "* **Regression** (continuous numbers): e.g., predicting *Glucose*, *BMI*, or *ER_visits*.\n",
        "* **Unsupervised Learning** (no target variable): e.g., finding patterns or groups in data.\n",
        "\n",
        "### 2. **Size and Shape of Data**\n",
        "\n",
        "* Small dataset (hundreds of rows) ‚Üí simpler algorithms (logistic regression, decision tree) often perform well.\n",
        "* Larger dataset (thousands‚Äìmillions) ‚Üí more complex algorithms (random forest, XGBoost, neural networks).\n",
        "\n",
        "### 3. **Interpretability vs. Accuracy**\n",
        "\n",
        "* If you need **interpretability** (clear reasoning, coefficients, feature importance):\n",
        "  \n",
        "  * Logistic Regression\n",
        "  * Decision Tree\n",
        "* If you want **higher predictive performance** (even if harder to interpret):\n",
        "  \n",
        "  * Random Forest\n",
        "  * Gradient Boosting (XGBoost, LightGBM)\n",
        "\n",
        "* * *\n",
        "\n",
        "### Algorithm Options by Task Type\n",
        "\n",
        "| Task Type                | Common Algorithms                                  | Notes                                                                 |\n",
        "| :----------------------- | :------------------------------------------------- | :-------------------------------------------------------------------- |\n",
        "| **Binary Classification**| Logistic Regression, SVM, Decision Tree, Random Forest, Gradient Boosting, Naive Bayes, K-Nearest Neighbors | Good for predicting one of two outcomes.                             |\n",
        "| **Multiclass Classification** | Logistic Regression (multinomial), SVM, Decision Tree, Random Forest, Gradient Boosting, Naive Bayes, K-Nearest Neighbors | Used when there are more than two discrete outcomes.                 |\n",
        "| **Regression**           | Linear Regression, Ridge, Lasso, Elastic Net, Decision Tree Regressor, Random Forest Regressor, Gradient Boosting Regressor, Support Vector Regression | For predicting continuous numerical values.                          |\n",
        "| **Unsupervised Learning**| K-Means Clustering, Hierarchical Clustering, DBSCAN, PCA, t-SNE, Association Rules (Apriori) | Used for finding hidden patterns, groups, or reducing dimensionality without a specific target variable. |\n",
        "\n",
        "* * *\n",
        "\n",
        "# üîπ Step 2: Recommendation for Your Dataset\n",
        "\n",
        "Your dataset has **1000 rows** with **mixed features** (numeric, categorical).\n",
        "\n",
        "* **Target variable:** `\"Presence_of_IBS\"` (binary classification).\n",
        "* **Goal:** Predict IBS status from the provided features.\n",
        "\n",
        "üëâ Best first choice: **Logistic Regression** (simple, interpretable baseline).\n",
        "üëâ Next step: Compare with **Random Forest** (handles nonlinear interactions, often more accurate)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ Step 1: Guidance on Algorithm Selection\n",
        "\n",
        "When deciding on a machine learning algorithm for your dataset, think about three key things:\n",
        "\n",
        "### 1. **Type of Outcome (Target Variable)**\n",
        "\n",
        "* **Binary Classification** (Yes/No, 0/1): e.g., *Presence_of_DM*, *Presence_of_HTN*.\n",
        "* **Multiclass Classification**: e.g., cancer staging, disease type.\n",
        "* **Regression** (continuous numbers): e.g., predicting *Glucose*, *BMI*, or *ER_visits*.\n",
        "\n",
        "### 2. **Size and Shape of Data**\n",
        "\n",
        "* Small dataset (hundreds of rows) ‚Üí simpler algorithms (logistic regression, decision tree) often perform well.\n",
        "* Larger dataset (thousands‚Äìmillions) ‚Üí more complex algorithms (random forest, XGBoost, neural networks).\n",
        "\n",
        "### 3. **Interpretability vs. Accuracy**\n",
        "\n",
        "* If you need **interpretability** (clear reasoning, coefficients, feature importance):\n",
        "\n",
        "  * Logistic Regression\n",
        "  * Decision Tree\n",
        "* If you want **higher predictive performance** (even if harder to interpret):\n",
        "\n",
        "  * Random Forest\n",
        "  * Gradient Boosting (XGBoost, LightGBM)\n",
        "\n",
        "---\n",
        "\n",
        "# üîπ Step 2: Recommendation for Your Dataset\n",
        "\n",
        "Your dataset has **1000 rows** with **mixed features** (numeric labs, binary comorbidities, categorical sex).\n",
        "\n",
        "* **Target variable candidate:** `\"Presence_of_DM\"` (binary classification).\n",
        "* **Goal:** Predict diabetes status from labs, demographics, and ER visits.\n",
        "\n",
        "üëâ Best first choice: **Logistic Regression** (simple, interpretable baseline).\n",
        "üëâ Next step: Compare with **Random Forest** (handles nonlinear interactions, often more accurate).\n",
        "\n"
      ],
      "metadata": {
        "id": "dXhtqbALU-aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns=[\"Presence_of_IBS\"])  # predictors\n",
        "y = df[\"Presence_of_IBS\"]                 # target\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = ['Sex_at_birth', 'Alcohol_Consumption', 'Smoking_Status', 'Exercise_Frequency']\n",
        "numerical_features = ['Age', 'BMI', 'Fiber_Intake', 'Stress_Level', 'Stool_Frequency']\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a column transformer to apply different transformations to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply preprocessing to the training and testing data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Get the feature names after one-hot encoding\n",
        "feature_names = numerical_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
        "\n",
        "# Convert processed data back to DataFrames with feature names (optional, but helpful for interpretation)\n",
        "X_train_scaled = pd.DataFrame(X_train_processed, columns=feature_names)\n",
        "X_test_scaled = pd.DataFrame(X_test_processed, columns=feature_names)"
      ],
      "metadata": {
        "id": "aXkuoke1VMCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is ready to test! Let's try Logistic Regression first."
      ],
      "metadata": {
        "id": "oiocr5b3VeD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_lr = log_reg.predict(X_test_scaled)\n",
        "\n",
        "print(\"=== Logistic Regression Results ===\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_lr)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No IBS\", \"IBS\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "blAwBbesVsna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Classification Report Terms\n",
        "\n",
        "* **Precision (Positive Predictive Value)**\n",
        "  *Of all the patients the model predicted as having diabetes, how many actually had diabetes?*\n",
        "  Formula: **TP / (TP + FP)**\n",
        "  ‚Üí High precision = few false positives.\n",
        "\n",
        "* **Recall (Sensitivity, True Positive Rate)**\n",
        "  *Of all the patients who actually had diabetes, how many did the model correctly identify?*\n",
        "  Formula: **TP / (TP + FN)**\n",
        "  ‚Üí High recall = few false negatives.\n",
        "  \n",
        "* **F1-score**\n",
        "  *The harmonic mean of precision and recall.*\n",
        "  Formula: **2 √ó (Precision √ó Recall) / (Precision + Recall)**\n",
        "  ‚Üí Useful when you want a balance between precision and recall.\n",
        "\n",
        "* **Support**\n",
        "  *The number of true cases for each class in the test dataset.*\n",
        "  ‚Üí Example: If 29 people really had diabetes, support for class ‚Äú1‚Äù is 29.\n",
        "\n",
        "* **Accuracy**\n",
        "  *The fraction of all predictions that were correct.*\n",
        "  Formula: **(TP + TN) / Total**\n",
        "  ‚Üí Can be misleading with imbalanced classes.\n",
        "\n",
        "* **Macro avg**\n",
        "  *Average of precision, recall, and F1 across all classes, treating each class equally (unweighted).*\n",
        "  ‚Üí Good for seeing performance on minority classes.\n",
        "\n",
        "* **Weighted avg**\n",
        "  *Average of precision, recall, and F1 across classes, weighted by the number of samples in each class (support).*\n",
        "  ‚Üí Dominated by majority class performance if classes are imbalanced."
      ],
      "metadata": {
        "id": "1vU0YOy_yyQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob = log_reg.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.2f})\")\n",
        "plt.plot([0,1],[0,1],'k--')  # diagonal line\n",
        "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
        "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
        "plt.title(\"ROC Curve - Logistic Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lKdbAI_suiXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's repeat so you have code for the Random Forest! (Other notebooks have additional algorithms!)"
      ],
      "metadata": {
        "id": "1RAfunYSxtqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test_scaled)\n",
        "\n",
        "print(\"=== Random Forest Results ===\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# Feature importance plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No IBS\", \"IBS\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=X_train_scaled.columns)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=feat_importances.sort_values(ascending=False), y=feat_importances.sort_values(ascending=False).index)\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zVcwYoodu4ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities for the positive class (IBS = 1)\n",
        "y_prob_rf = rf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Compute ROC curve values\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob_rf)\n",
        "auc = roc_auc_score(y_test, y_prob_rf)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"Random Forest (AUC = {auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")  # diagonal = no skill\n",
        "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
        "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
        "plt.title(\"ROC Curve - Random Forest\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gpiAbZmQxHee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTLDnreizogp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d5ba30a"
      },
      "source": [
        "# Task\n",
        "Enable users to download the preprocessing file with data field names and specifications and the model. Provide code for a streamlit app that will let users upload their preprocessing file and model file to populate the streamlit app so that new data can be entered and predictions seen. This should support either the logistic regression or random forest examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14518ae1"
      },
      "source": [
        "## Save preprocessing and model files\n",
        "\n",
        "### Subtask:\n",
        "Add code to save the fitted `preprocessor` object and the trained `log_reg` and `rf` models to files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbac6456"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires saving the fitted preprocessor and trained models. The `joblib` library is suitable for this purpose. I will save the `preprocessor`, `log_reg`, and `rf` objects to `.joblib` files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de1691f9"
      },
      "source": [
        "import joblib\n",
        "\n",
        "# Save the preprocessor\n",
        "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
        "\n",
        "# Save the Logistic Regression model\n",
        "joblib.dump(log_reg, 'logistic_regression_model.joblib')\n",
        "\n",
        "# Save the Random Forest model\n",
        "joblib.dump(rf, 'random_forest_model.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulations! We have a model! Two models even, a random forest and the original logistic regression.\n",
        "\n",
        "But, what can we do with them?\n",
        "\n",
        "Well, what if we created an online app so users could enter values and see the predictions? This is these models really come to life!\n",
        "\n",
        "Click [here](https://fsm-ibs-predict.streamlit.app/) for the Website (free hosted - so watch screen first)."
      ],
      "metadata": {
        "id": "A5zdHtUNC5GG"
      }
    }
  ]
}